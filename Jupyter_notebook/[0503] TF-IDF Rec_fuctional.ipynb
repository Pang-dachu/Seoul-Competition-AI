{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f790a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import joblib\n",
    "from joblib import dump\n",
    "from konlpy.tag import Mecab\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99894c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 업데이트 함수 \n",
    "\n",
    "'''\n",
    "라이브러리 사용 \n",
    "pandas, requests, json, re \n",
    "mecab\n",
    "\n",
    "- get_dataframe(API_KEY : str , DATA_URL : str)\n",
    "- concat_data(data1, data2 )\n",
    "- clean_sentence(sentence)\n",
    "- tokenize(original_sent)\n",
    "\n",
    "'''\n",
    "def get_dataframe(API_KEY : str , DATA_URL : str) :\n",
    "    ''' \n",
    "    - 초기 모델 생성을 위하여 공공데이터를 json 형태로 받아와 데이터프레임으로 생성.\n",
    "    - 이후 백엔드에서 데이터를 받아오는 과정으로 변경시 사용하지 않음.\n",
    "    \n",
    "    API_KEY : 공공 데이터의 API를 받아오기 위한 개인 키 값.\n",
    "    DATA_URL : 공공 데이터의 API URL 값 \n",
    "    \n",
    "    return : pd.DataFrame \n",
    "    \n",
    "    '''\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    # 요청 및 json 데이터 변환 \n",
    "    response = requests.get(DATA_URL)\n",
    "    response_data = response.content.decode()\n",
    "    json_data = json.loads(response_data)\n",
    "    \n",
    "    # 데이터 row 갯수 확인\n",
    "    # 데이터는 한번에 1000개 단위로만 요청이 가능함.\n",
    "    list_total_count = json_data[list( json_data.keys() )[0]]['list_total_count']\n",
    "    \n",
    "    # 요청 갯수 제한에 따른 반복 실행하여 데이터 프레임 생성\n",
    "    count = list_total_count // 1000\n",
    "    les_count = list_total_count % 1000\n",
    "\n",
    "    # 반복을 통한 데이터 프레임 생성 \n",
    "    # 1000개 단위\n",
    "    for i in range(count) :\n",
    "        temp_url = DATA_URL[:-4]+str(1000*i + 1) + \"/\" + str(1000*(i+1)) \n",
    "        response = requests.get(temp_url)\n",
    "\n",
    "        temp_data = response.content.decode()\n",
    "        json_data = json.loads(temp_data)\n",
    "\n",
    "        temp_df = pd.json_normalize(json_data[list( json_data.keys() )[0]]['row'])\n",
    "        data = pd.concat( [data, temp_df] )\n",
    "    \n",
    "    # 1000개 단위로 반복 이후 나머지 갯수에 대한 추가 처리 \n",
    "    temp_url =  DATA_URL[:-4]+str(1000*count + 1) + \"/\" + str(1000*count + les_count) \n",
    "    response = requests.get(temp_url)\n",
    "\n",
    "    temp_data = response.content.decode()\n",
    "    json_data = json.loads(temp_data)\n",
    "    \n",
    "    temp_df = pd.json_normalize(json_data[list( json_data.keys() )[0]]['row'])\n",
    "    \n",
    "    # 1000개 단위, 나머지 단위에 대한 데이터 병합\n",
    "    data = pd.concat( [data, temp_df] )\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def concat_data(data1, data2 ) : \n",
    "    '''\n",
    "    - 초기 모델 생성을 위하여 공공데이터를 json 형태로 받아와 데이터프레임으로 생성.\n",
    "    - 이후 백엔드에서 데이터를 받아오는 과정으로 변경시 사용하지 않음.\n",
    "    - 초기에 사용하는 데이터가 2개이므로 병합하는 과정이 필요했음.\n",
    "    \n",
    "    data1 : DataFrame\n",
    "    data2 : DataFrame\n",
    "    \n",
    "    return : pd.DataFrame \n",
    "    \n",
    "    '''\n",
    "    # 컬럼명 통일 시키는 과정\n",
    "    data1.columns = ['교육넘버', '교육명', '교육신청시작일', '교육신청종료일', '교육시작일', '교육종료일', \"수업시간\", '수강정원', '교육상태', '교육비용', '강좌상세화면']\n",
    "    data2.columns = [\"교육넘버\", \"교육명\", \"교육시작일\", \"교육종료일\", \"교육신청시작일\", \"교육신청종료일\", \"수강정원\", \"교육비용\", \"교육상태\", \"강좌상세화면\"]\n",
    "    \n",
    "    # 컬럼명 순서 통일 \n",
    "    col_sort = ['교육넘버', '교육명', '교육신청시작일', '교육신청종료일', '교육시작일', '교육종료일',  '수강정원','교육상태', '교육비용', '강좌상세화면']\n",
    "    \n",
    "    data_1 = data1[ col_sort ]\n",
    "    data_2 = data2[ col_sort ]\n",
    "    # 이후 concat 진행 \n",
    "    data = pd.concat([data_1, data_2])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def date_preprocessing(dataframe) :\n",
    "    ''' \n",
    "    - 두 개의 데이터 프레임이 날짜 표현을 서로 다른 방식으로 표현함\n",
    "    - 신청 가능한 교육을 날짜 기준으로 선정할 예정이므로 datetime을 사용하기 위해 날짜 형식 변경\n",
    "    - 이후 백엔드에서 데이터를 받아오는 경우 수정되거나 사용되지 않을 수 있음.\n",
    "    \n",
    "    dataframe  : dataframe\n",
    "    \n",
    "    return : dataframe\n",
    "    \n",
    "    '''\n",
    "    ## 날짜 정보 datetime\n",
    "\n",
    "    # 표현 형식 변경\n",
    "    dataframe[\"교육신청시작일\"] = dataframe[\"교육신청시작일\"].apply(lambda x : re.sub(r\"\\.\", r\"-\", x) )\n",
    "    dataframe[\"교육신청종료일\"] = dataframe[\"교육신청종료일\"].apply(lambda x : re.sub(r\"\\.\", r\"-\", x) )\n",
    "    dataframe[\"교육시작일\"] = dataframe[\"교육시작일\"].apply(lambda x : re.sub(r\"\\.\", r\"-\", x) )\n",
    "    dataframe[\"교육종료일\"] = dataframe[\"교육종료일\"].apply(lambda x : re.sub(r\"\\.\", r\"-\", x) )\n",
    "    \n",
    "    # int, datetime 형태 변경 \n",
    "    date_trans_col = [\"교육신청시작일\",\"교육신청종료일\",\"교육시작일\",\"교육종료일\"]\n",
    "\n",
    "    for col in date_trans_col : \n",
    "        dataframe[col] = pd.to_datetime( dataframe[col] )\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "# 불용어 처리 \n",
    "def clean_sentence(sentence) :\n",
    "    '''\n",
    "    - 데이터 분석 이후 문장에서 의미가 없을 것으로 판단되는 단어 불용어로 판단하여 처리 \n",
    "    \n",
    "    sentence : Series\n",
    "    \n",
    "    return : Series\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 날짜, 기수, 차수 제거 \n",
    "    sentence = re.sub(r\"[0-9]+년\", r\" \", sentence)\n",
    "    sentence = re.sub(r\"[0-9]+차\", r\" \", sentence)\n",
    "    sentence = re.sub(r\"[0-9]+기\", r\" \", sentence)\n",
    "    sentence = re.sub(r\"[0-9]+월\", r\" \", sentence)\n",
    "    sentence = re.sub(r\"[0-9]+일\", r\" \", sentence)\n",
    "    sentence = re.sub(r\"[0-9]{1,2}.[0-9]{1,2}\", r\" \", sentence)\n",
    "    \n",
    "    # (주) , (요일)\n",
    "    sentence = re.sub(r\"\\(+[가-힣]+\\)\", r\" \", sentence)\n",
    "    sentence = re.sub(r\"[가-힣]째주\", r\" \", sentence) \n",
    "    sentence = re.sub(r\"[가-힣]{1}요일\", r\" \", sentence)\n",
    "\n",
    "    # 마감 키워드 필요 없음\n",
    "    sentence = re.sub(r\"마감\", r\" \", sentence)\n",
    "    \n",
    "    # 50이라는 숫자 필요 없음 \n",
    "    sentence = re.sub(r\"50\", r\" \", sentence)\n",
    "    # 자격증 n급 필요 없을듯 \n",
    "    sentence = re.sub(r\"[0-9]+급\", r\" \", sentence)\n",
    "    # n단계도 필요 없을듯 \n",
    "    sentence = re.sub(r\"[0-9]+단계\", r\" \", sentence)\n",
    "    sentence = re.sub(r\"[^0-9가-힣a-zA-Z]\", r\" \", sentence)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "def tokenize(original_sent):\n",
    "    '''\n",
    "    - Mecab 형태소 분석기를 사용하여 문장를 \"명사\" 단위로 분류\n",
    "    - 현 데이터는 문장의 의미보다는 사용되는 핵심 단어가 중요할 것으로 판단하여 결정\n",
    "    \n",
    "    sentence : Series\n",
    "    \n",
    "    return : Series\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    tokenizer = Mecab()\n",
    "\n",
    "    # tokenizer를 이용하여 original_sent를 토큰화하여 tokenized_sent에 저장하고, 이를 반환합니다.\n",
    "    sentence = original_sent.replace('\\n', '').strip()\n",
    "    \n",
    "    # tokenizer.nouns(sentence) -> 명사만 추출\n",
    "    tokens = tokenizer.nouns(sentence)\n",
    "    \n",
    "    tokens = ' '.join(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def data_preprocessing(dataframe) :\n",
    "    '''\n",
    "    -  정의된 불용어 처리, 토크나이저를 데이터에 적용\n",
    "    \n",
    "    dataframe : dataframe\n",
    "    \n",
    "    return : dataframe\n",
    "    \n",
    "    '''\n",
    "    # 교육명 불용어 처리하여 clean_sentence 컬럼으로 생성\n",
    "    dataframe[\"clean_sentence\"] = dataframe[\"교육명\"].apply(lambda x : clean_sentence(x) )\n",
    "    \n",
    "    # 교육명 mecab 명사 토크나이징하여 mecab 컬럼으로 생성\n",
    "    dataframe[\"mecab\"] = dataframe[\"clean_sentence\"].apply(lambda x : tokenize(x) ) \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def save_model(data) :\n",
    "    '''\n",
    "    -  전체 데이터에 대한 tf-idf 모델 생성 후 저장 \n",
    "    \n",
    "    data : dataframe\n",
    "    \n",
    "    '''\n",
    "    tfidf_vector = TfidfVectorizer().fit( data[\"mecab\"] )\n",
    "    dump(tfidf_vector, 'tfidf.pkl')\n",
    "    \n",
    "\n",
    "def save_dataframe(data) :\n",
    "    \n",
    "    data.to_pickle('data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f60d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력데이터 처리 함수 \n",
    "\n",
    "# 정규화 \n",
    "def l1_normalize(vector):\n",
    "    '''\n",
    "    - 코사인 유사도를 구하기 이전 범위 조정을 위함\n",
    "\n",
    "    vector : tf-idf vector\n",
    "\n",
    "    return : l1_normalize 결과 \n",
    "\n",
    "    '''\n",
    "    norm = np.sum(vector)\n",
    "    return vector / norm\n",
    "\n",
    "def cosine_similarity_value(vec_1, vec_2):\n",
    "    '''\n",
    "    - 코사인 유사도 계산\n",
    "\n",
    "    vec_1 : l1_normalize tf-idf vector\n",
    "    vec_2 : l1_normalize tf-idf vector\n",
    "\n",
    "    return : 코사인 유사도 계산 결과 \n",
    "\n",
    "    '''\n",
    "    return round(cosine_similarity(vec_1, vec_2)[0][0], 3)\n",
    "\n",
    "def possible_edu (dataframe) :\n",
    "    '''\n",
    "    - 날짜 기준으로 가능한 교육 \n",
    "\n",
    "    dataframe : dataframe\n",
    "    \n",
    "    return : dataframe\n",
    "\n",
    "    '''\n",
    "    today = f\"{dt.today().year}-{dt.today().month}-{dt.today().day}\"\n",
    "    \n",
    "    # 수강 신청이 가능한 경우 \n",
    "    # 1. 교육 상태가 마감이 아닌 경우 \n",
    "    cond_01 = (dataframe[\"교육상태\"] == \"마감\")\n",
    "\n",
    "    # 2. 교육 신청 종료일이 현재 날짜를 지나지 않은 경우\n",
    "    cond_02 = (dataframe[\"교육신청종료일\"] > today)\n",
    "    \n",
    "    temp_data = dataframe.loc[ ~cond_01 & cond_02 ]\n",
    "\n",
    "    return temp_data\n",
    "\n",
    "def load_model() :\n",
    "    '''\n",
    "    - 생성되어 있는 tf-idf 모델을 pickle 형태로 load \n",
    "    \n",
    "    return : tf-idf model\n",
    "\n",
    "    '''       \n",
    "    model = joblib.load(\"tfidf.pkl\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_dataframe() :\n",
    "    \n",
    "    data = pd.read_pickle('data.pkl')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def edu_recommend(input_data, data, vectorizer) :\n",
    "    '''\n",
    "    - 입력한 단어에 대하여 유사한 교육 추천 \n",
    "    - 백엔드와 연결 이후 return 값을 아이디로 변경 \n",
    "    \n",
    "    input_data : str\n",
    "    data : dataframe\n",
    "    vectorizer : TfidfVectorizer\n",
    "    \n",
    "    return : str\n",
    "\n",
    "    '''   \n",
    "    # 입력 단어에 대한 임시 데이터 프레임 생성    \n",
    "    temp = pd.DataFrame({\n",
    "        # \"교육넘버\" : \"0000\",\n",
    "        \"교육명\": [input_data],\n",
    "        \"clean_sentence\" : clean_sentence(input_data),\n",
    "         \"mecab\" : [\"123\"]\n",
    "    })\n",
    "\n",
    "    temp[\"mecab\"] = temp[\"clean_sentence\"].apply(lambda x : tokenize(x) )\n",
    "    \n",
    "    # 검색 단어를 포함한 전체 데이터 프레임 \n",
    "    temp_total_data = data[::]\n",
    "    \n",
    "    temp_total_data = pd.concat([temp_total_data,temp])\n",
    "    temp_total_data = temp_total_data.reset_index( drop=True )\n",
    "    \n",
    "    # TF-IDF 벡터화 \n",
    "    tfidf_vector = vectorizer.transform( temp_total_data[\"mecab\"] )\n",
    "    tfidf_norm_l1 = l1_normalize(tfidf_vector)\n",
    "    \n",
    "    # 검색 단어 \n",
    "    target = tfidf_norm_l1[-1]\n",
    "    \n",
    "    # 코사인 유사도 적용\n",
    "    cosin_result = []\n",
    "\n",
    "    for i in tfidf_norm_l1 :\n",
    "        cosin_result.append( cosine_similarity_value(target, i) )\n",
    "        \n",
    "    temp_total_data[\"cosin\"] = cosin_result\n",
    "\n",
    "    temp = temp_total_data.loc[ temp_total_data[\"cosin\"] > 0 ]\n",
    "    temp = temp.sort_values([\"cosin\"], ascending=False)[1:6]\n",
    "    \n",
    "    if temp.empty :\n",
    "            print(\"추천 정보가 없습니다.\")\n",
    "            return None\n",
    "\n",
    "    for i,j in zip(temp[\"교육명\"], temp[\"cosin\"]):\n",
    "        print( i, j )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d880b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_update() :\n",
    "    '''\n",
    "    - 모델 업데이트 \n",
    "    - 일정 주기로 업데이트 시 실행할 수 있도록 \n",
    "    - .py 파일을 따로 생성하는 방법도 고려 \n",
    "    \n",
    "    input_data : str\n",
    "    data : dataframe\n",
    "    vectorizer : TfidfVectorizer\n",
    "    \n",
    "    return : str\n",
    "\n",
    "    '''   \n",
    "\n",
    "    API_KEY = \"61484f6245666f7838344a79694e77\"\n",
    "\n",
    "    서울시50플러스포털교육정보 = f\"http://openapi.seoul.go.kr:8088/{API_KEY}/json/FiftyPotalEduInfo/1/5/\"\n",
    "    서울시어르신취업지원센터교육정보 = f\"http://openapi.seoul.go.kr:8088/{API_KEY}/json/tbViewProgram/1/5/\"\n",
    "    \n",
    "    data_01 = get_dataframe(API_KEY, 서울시50플러스포털교육정보)\n",
    "    data_02 = get_dataframe(API_KEY, 서울시어르신취업지원센터교육정보)\n",
    "    total_data = concat_data(data_01, data_02)\n",
    "    \n",
    "    total_data = date_preprocessing(total_data)\n",
    "    total_data = data_preprocessing(total_data)\n",
    "    \n",
    "    save_model(total_data)\n",
    "    save_dataframe(total_data)\n",
    "    \n",
    "    print(\"모델 업데이트 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cdce267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 업데이트 완료\n",
      "CPU times: user 1.51 s, sys: 1.74 s, total: 3.26 s\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "654c9ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.5 ms, sys: 0 ns, total: 24.5 ms\n",
      "Wall time: 34.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = load_model()\n",
    "data = load_dataframe()\n",
    "data = possible_edu(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e353806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력해주세요 : 코딩\n",
      "[2023채움학교] 중장년이 알아야 할 코딩기술 (특강) 0.557\n",
      "[맞춤형 일자리]창의융합메이커코딩 전문강사 양성 0.368\n",
      "[50+스마트] 내가 조종하는 미니로봇! '블록코딩 체험과정' 0.357\n",
      "CPU times: user 90.4 ms, sys: 6.9 ms, total: 97.3 ms\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_data = input(\"검색어를 입력해주세요 : \")\n",
    "\n",
    "edu_recommend(input_data, data, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af60528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
